{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ed7741",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Classification\n",
    "\n",
    "This file starts with the data images file saved from an iphone 15 pro, which is capable of of taking 'portrait' photos which include depth map information.\n",
    "\n",
    "We generate a review file containing a list of all the images.  \n",
    "\n",
    "We generate a mapping file.  The file list is used to generate a mapping file to rename and effectively label the images.  Note that the images were taken in a specific order to aid labelling.\n",
    "\n",
    "\n",
    "\n",
    "The renamed files are then sorted into relevant folders dependent on their name content.  Some explanation:\n",
    "\n",
    "gy_      greyscale, 1-dimensional/channel black and white photos\n",
    "or_      original, 3-dimensional/channel colour (red green blue, RGB) photos\n",
    "fc_      four-channel; 4-dimensional/channel consisting RGB with an additional depth map layer\n",
    "\n",
    "\n",
    "\n",
    "CLASSIFICATIONS\n",
    "\n",
    "There are 4 classification models in the data\n",
    "\n",
    "country\n",
    "piece\n",
    "exact_piece\n",
    "force\n",
    "\n",
    "\n",
    "LOCALISATION\n",
    "\n",
    "This will mainly relate to the locations of the folders and files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a1d88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "086fcccb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98356bc6",
   "metadata": {},
   "source": [
    "# Part 1 : Summary List of image files\n",
    "\n",
    "The following creates an excel which summarises the content of the image file folder.  This file is used to analyse the image files.\n",
    "\n",
    "OFFLINE a mapping file of current_name/target_name is generated to copy/rename files in part 3.\n",
    "\n",
    "LOCALISATION\n",
    "\n",
    "directory_path   this is the target location where all the images from the phone are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def create_file_list(directory):\n",
    "    # Path to the directory where the Excel file will be saved\n",
    "    subfolder_path = os.path.join(directory, 'FileList')\n",
    "    \n",
    "    # Create the subfolder if it does not exist\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "    \n",
    "    # List to store file details\n",
    "    file_details = []\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            # Get the file modification time\n",
    "            mod_time = os.path.getmtime(filepath)\n",
    "            # Convert the modification time to a human-readable format\n",
    "            mod_time = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Get relative path of file\n",
    "            relative_path = os.path.relpath(root, directory)\n",
    "            # Get file extension, if file has one\n",
    "            extension = os.path.splitext(file)[1] if os.path.splitext(file)[1] else 'No extension'\n",
    "            # Append the details to the list\n",
    "            file_details.append({\n",
    "                'Filename': file,\n",
    "                'Folder': relative_path,\n",
    "                'Modification Date': mod_time,\n",
    "                'Type': extension\n",
    "            })\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(file_details)\n",
    "    \n",
    "    # Current timestamp for file naming\n",
    "    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save to Excel with timestamp\n",
    "    excel_path = os.path.join(subfolder_path, f'file_list_{current_time}.xlsx')\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f'Excel file has been saved to {excel_path}')\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\00.Imported_All_Files\\202407__'\n",
    "create_file_list(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83c470f",
   "metadata": {},
   "source": [
    "# run to generate :\n",
    "Excel file has been saved to C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\00.Imported_All_Files\\202407__\\FileList\\file_list_20240729_125635.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb2290",
   "metadata": {},
   "source": [
    "# Part 2: collect only the jpg files\n",
    "\n",
    "There are 4 formats of image file per photo, jpg (original), jpg (enhanced), 2 x aae files.  We need the original jpg, copied to new folder for repeatability of process.\n",
    "\n",
    "LOCALISATION\n",
    "\n",
    "source_directory            this is the target location where all the images from the phone are located\n",
    "destination_directory       this is the destination location folder for the jpg only files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd704b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "def copy_filtered_images(source_dir, dest_dir):\n",
    "    # Regex to match files like IMG_####.JPG\n",
    "    file_pattern = re.compile(r'^IMG_\\d{4}\\.JPG$', re.IGNORECASE)\n",
    "\n",
    "    # Counter for copied files\n",
    "    copied_files_count = 0\n",
    "\n",
    "    # Ensure the destination directory exists, if not, create it\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    # List all files in the source directory (only the root, ignoring subdirectories)\n",
    "    for filename in os.listdir(source_dir):\n",
    "        # Full path of the file\n",
    "        file_path = os.path.join(source_dir, filename)\n",
    "\n",
    "        # Check if it is a file and matches the desired pattern\n",
    "        if os.path.isfile(file_path) and file_pattern.match(filename):\n",
    "            # Construct the destination path\n",
    "            dest_path = os.path.join(dest_dir, filename)\n",
    "            # Copy the file to the destination directory\n",
    "            shutil.copy(file_path, dest_path)\n",
    "            copied_files_count += 1\n",
    "\n",
    "    return copied_files_count\n",
    "\n",
    "# Source and destination directories\n",
    "source_directory = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\00.Imported_All_Files\\202407__'\n",
    "destination_directory = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\01.Imported_jpg_Files'\n",
    "\n",
    "# Perform the copy operation\n",
    "copied_count = copy_filtered_images(source_directory, destination_directory)\n",
    "print(f'Total files copied: {copied_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a434cdb",
   "metadata": {},
   "source": [
    "Total files copied: 1530"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f501a62",
   "metadata": {},
   "source": [
    "# Part 3:  ReName and ReLabel\n",
    "\n",
    "This copies and renames/labels the original jpg files.\n",
    "This uses a mapping file generated from the list in part 1.\n",
    "\n",
    "LOCALISATION\n",
    "\n",
    "source_directory         this is where the unlabelled jpg files are\n",
    "\n",
    "destination_directory    this is where the labelled jpg files will go\n",
    "\n",
    "excel_file_path          this is the exact file used for file mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ceb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def rename_and_move_files(source_dir, dest_dir, excel_path):\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # Create the destination directory if it does not exist\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    # Rename and move the files based on the mapping in the DataFrame\n",
    "    renamed_files_count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        current_filename = row['Current Filename']\n",
    "        new_filename = row['New Filename']\n",
    "        current_file_path = os.path.join(source_dir, current_filename)\n",
    "        \n",
    "        # Check if the file exists and new filename is provided\n",
    "        if os.path.exists(current_file_path) and pd.notna(new_filename):\n",
    "            new_file_path = os.path.join(dest_dir, new_filename)\n",
    "            # Move and rename the file\n",
    "            os.rename(current_file_path, new_file_path)\n",
    "            renamed_files_count += 1\n",
    "\n",
    "    return renamed_files_count\n",
    "\n",
    "# Directory containing the files\n",
    "source_directory = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\01.Imported_jpg_Files'\n",
    "# Destination directory for renamed files\n",
    "destination_directory = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\02.Labelled_jpg'\n",
    "# Path to the Excel file with the renaming mapping\n",
    "excel_file_path = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\00.Imported_All_Files\\202407__\\FileList\\file_mapping.xlsx'\n",
    "\n",
    "# Perform the renaming and moving operation\n",
    "renamed_count = rename_and_move_files(source_directory, destination_directory, excel_file_path)\n",
    "print(f'Total files renamed and moved: {renamed_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce2995",
   "metadata": {},
   "source": [
    "Total files renamed and moved: 1530"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b0838",
   "metadata": {},
   "source": [
    "# Part 4 : Extract and unify layers\n",
    "\n",
    "Extract all the layers and unify as 4284h x 5712w (blow up depth map)\n",
    "\n",
    "Extraction uses exiftool\n",
    "\n",
    "All jpg files converted to png as jpg cannot handle >3 channels\n",
    "\n",
    "Resizing to blow up the depth map layer to the size of the other layers (4284 x 5712) to allow it to be treated the same as other images.\n",
    "\n",
    "More than just the later processed images are produced:\n",
    "    image                   original\n",
    "    image2                  scaled down original (assumed used for thumbnails)\n",
    "    image3                  single depth map\n",
    "    blue_channel            single blue channel\n",
    "    green_channel           single green channel\n",
    "    red_channel             single red channel\n",
    "    grayscale_image         original greyscale image\n",
    "    upscaled_image3         blown-up / upscaled_image3 (depth map)\n",
    "    four channel image      combination of original and upscaled_image3\n",
    "\n",
    "WARNING\n",
    "\n",
    "This code, with the displayed images in the log, take 4+ hours to run\n",
    "\n",
    "LOCALISATION\n",
    "\n",
    "source_directory             where the labelled jpg are\n",
    "destination_directory        where the extracted files will go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c8bbb",
   "metadata": {},
   "source": [
    "# WARNING take 4 + hours to run if you display the images as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042657a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import piexif\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Define source and destination directories\n",
    "source_directory = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\02.Labelled_jpg'\n",
    "destination_directory = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\03.ExtractedLayers'\n",
    "os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "# Define the target size for the basic channels\n",
    "TARGET_SIZE = (4284, 5712)\n",
    "\n",
    "def run_exiftool(image_path, output_dir, timestamp, base_name):\n",
    "    print(\"Extracting MP images using exiftool...\")\n",
    "    mp_image2_path = os.path.join(output_dir, f\"{timestamp}_{base_name}_mp_image2.png\")\n",
    "    mp_image3_path = os.path.join(output_dir, f\"{timestamp}_{base_name}_mp_image3.png\")\n",
    "    \n",
    "    try:\n",
    "        # Extract MPImage2\n",
    "        result = subprocess.run([\"exiftool\", \"-b\", \"-MPImage2\", image_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        with open(mp_image2_path, 'wb') as f:\n",
    "            f.write(result.stdout)\n",
    "        print(f\"MPImage2 saved to {mp_image2_path}\")\n",
    "        \n",
    "        # Extract MPImage3\n",
    "        result = subprocess.run([\"exiftool\", \"-b\", \"-MPImage3\", image_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        with open(mp_image3_path, 'wb') as f:\n",
    "            f.write(result.stdout)\n",
    "        print(f\"MPImage3 saved to {mp_image3_path}\")\n",
    "\n",
    "        # Check if files were created\n",
    "        if not os.path.exists(mp_image2_path) or not os.path.exists(mp_image3_path):\n",
    "            raise FileNotFoundError(\"Extracted MP images not found.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during exiftool extraction: {e.stderr.decode()}\")\n",
    "        return None, None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "    return mp_image2_path, mp_image3_path\n",
    "\n",
    "def load_image(image_path):\n",
    "    print(\"Loading image...\")\n",
    "    start_time = time.time()\n",
    "    image = Image.open(image_path)\n",
    "    end_time = time.time()\n",
    "    print(f\"Loading image took {end_time - start_time:.2f} seconds\")\n",
    "    return image\n",
    "\n",
    "def check_orientation(image):\n",
    "    print(\"Checking and correcting image orientation...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        exif_data = piexif.load(image.info['exif'])\n",
    "        orientation = exif_data['0th'][piexif.ImageIFD.Orientation]\n",
    "        if orientation == 3:\n",
    "            image = image.rotate(180, expand=True)\n",
    "        elif orientation == 6:\n",
    "            image = image.rotate(270, expand=True)\n",
    "        elif orientation == 8:\n",
    "            image = image.rotate(90, expand=True)\n",
    "    except (KeyError, AttributeError):\n",
    "        pass\n",
    "    end_time = time.time()\n",
    "    print(f\"Orientation check took {end_time - start_time:.2f} seconds\")\n",
    "    return image\n",
    "\n",
    "def correct_orientation_opencv(image, orientation):\n",
    "    if orientation == 3:\n",
    "        image = cv2.rotate(image, cv2.ROTATE_180)\n",
    "    elif orientation == 6:\n",
    "        image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif orientation == 8:\n",
    "        image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    return image\n",
    "\n",
    "def convert_to_opencv(image):\n",
    "    print(\"Converting image to OpenCV format...\")\n",
    "    start_time = time.time()\n",
    "    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    end_time = time.time()\n",
    "    print(f\"Conversion took {end_time - start_time:.2f} seconds\")\n",
    "    return image_cv\n",
    "\n",
    "def extract_rgb_channels(image_cv):\n",
    "    print(\"Extracting RGB channels...\")\n",
    "    start_time = time.time()\n",
    "    blue_channel, green_channel, red_channel = cv2.split(image_cv)\n",
    "    end_time = time.time()\n",
    "    print(f\"RGB extraction took {end_time - start_time:.2f} seconds\")\n",
    "    return blue_channel, green_channel, red_channel\n",
    "\n",
    "def upscale_image(image, target_size):\n",
    "    print(\"Upscaling image...\")\n",
    "    start_time = time.time()\n",
    "    upscaled_image = cv2.resize(image, target_size, interpolation=cv2.INTER_CUBIC)\n",
    "    end_time = time.time()\n",
    "    print(f\"Upscaling took {end_time - start_time:.2f} seconds\")\n",
    "    return upscaled_image\n",
    "\n",
    "def normalize_image(image):\n",
    "    return cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "def report_image_sizes(images):\n",
    "    base_height, base_width = images[0].shape[:2]\n",
    "    for i, img in enumerate(images):\n",
    "        height, width = img.shape[:2]\n",
    "        if height != base_height or width != base_width:\n",
    "            print(f\"Image {i} has shape {img.shape[:2]}, expected ({base_height}, {base_width})\")\n",
    "    return base_height, base_width\n",
    "\n",
    "def save_images(image_cv, image2, image3, blue_channel, green_channel, red_channel, grayscale_image, upscaled_image3, original_filename, save_directory, timestamp):\n",
    "    start_time = time.time()\n",
    "    print(\"Saving images...\")\n",
    "    base_name = os.path.splitext(original_filename)[0]\n",
    "\n",
    "    # Create filenames with timestamp\n",
    "    filenames = [\n",
    "        f'{timestamp}_{base_name}_original.png',\n",
    "        f'{timestamp}_{base_name}_image2.png',\n",
    "        f'{timestamp}_{base_name}_image3.png',\n",
    "        f'{timestamp}_{base_name}_blue_channel.png',\n",
    "        f'{timestamp}_{base_name}_green_channel.png',\n",
    "        f'{timestamp}_{base_name}_red_channel.png',\n",
    "        f'{timestamp}_{base_name}_grayscale.png',\n",
    "        f'{timestamp}_{base_name}_upscaled_image3.png'\n",
    "    ]\n",
    "\n",
    "    # Save the images with the new filenames\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[0]), image_cv)\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[1]), image2)\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[2]), image3)\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[3]), blue_channel)\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[4]), green_channel)\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[5]), red_channel)\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[6]), grayscale_image)\n",
    "    cv2.imwrite(os.path.join(save_directory, filenames[7]), upscaled_image3)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Saving images took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "def display_images(image_cv, image2, image3, blue_channel, green_channel, red_channel, grayscale_image, upscaled_image3):\n",
    "    start_time = time.time()\n",
    "    print(\"Displaying images...\")\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    axes[0, 0].imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(image2, cmap='gray')\n",
    "    axes[0, 1].set_title('Image 2')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(image3, cmap='gray')\n",
    "    axes[0, 2].set_title('Image 3 (Depth Map)')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[0, 3].imshow(grayscale_image, cmap='gray')\n",
    "    axes[0, 3].set_title('Grayscale Image')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(blue_channel, cmap='gray')\n",
    "    axes[1, 0].set_title('Blue Channel')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(green_channel, cmap='gray')\n",
    "    axes[1, 1].set_title('Green Channel')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(red_channel, cmap='gray')\n",
    "    axes[1, 2].set_title('Red Channel')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    axes[1, 3].imshow(upscaled_image3, cmap='gray')\n",
    "    axes[1, 3].set_title('Upscaled Image 3')\n",
    "    axes[1, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    end_time = time.time()\n",
    "    print(f\"Displaying images took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "def merge_channels_to_4_channel(image_cv, depth_map):\n",
    "    print(\"Merging channels to create a 4-channel image...\")\n",
    "    start_time = time.time()\n",
    "    # Convert the RGB image to a numpy array\n",
    "    rgb_array = np.array(image_cv)\n",
    "    \n",
    "    # Ensure the depth map has the same height and width as the RGB image\n",
    "    if rgb_array.shape[:2] != depth_map.shape[:2]:\n",
    "        raise ValueError(\"RGB image and depth map must have the same dimensions\")\n",
    "\n",
    "    # Normalize all channels\n",
    "    rgb_array = normalize_image(rgb_array)\n",
    "    depth_map = normalize_image(depth_map)\n",
    "\n",
    "    # Expand the depth map to match the shape of the RGB array (height, width, 1)\n",
    "    depth_map_expanded = np.expand_dims(depth_map, axis=2)\n",
    "    \n",
    "    # Concatenate the RGB array with the depth map to form a 4-channel image\n",
    "    four_channel_image = np.concatenate((rgb_array, depth_map_expanded), axis=2)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Merging channels took {end_time - start_time:.2f} seconds\")\n",
    "    return four_channel_image\n",
    "\n",
    "def save_four_channel_image(four_channel_image, original_filename, save_directory, timestamp):\n",
    "    print(\"Saving the 4-channel image...\")\n",
    "    base_name = os.path.splitext(original_filename)[0]\n",
    "    filename = f'{timestamp}_{base_name}_four_channel.png'\n",
    "    save_path = os.path.join(save_directory, filename)\n",
    "    cv2.imwrite(save_path, four_channel_image)\n",
    "    print(f\"4-channel image saved to {save_path}\")\n",
    "\n",
    "def main(image_path, save_directory):\n",
    "    total_start_time = time.time()\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    \n",
    "    # Get the current timestamp\n",
    "    timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    # Extract base name of the file\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "    # Step 1: Extract MP images using exiftool\n",
    "    image2_path, image3_path = run_exiftool(image_path, save_directory, timestamp, base_name)\n",
    "    if not image2_path or not image3_path:\n",
    "        print(\"Error: Could not extract MP images.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Load and process the main image\n",
    "    image = load_image(image_path)\n",
    "    image = check_orientation(image)\n",
    "    image_cv = convert_to_opencv(image)\n",
    "    \n",
    "    # Step 3: Load image2 and image3\n",
    "    image2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image3 = cv2.imread(image3_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image2 is None or image3 is None:\n",
    "        print(\"Error: Could not load one or both additional images.\")\n",
    "        return\n",
    "    \n",
    "    # Get the orientation from the original image\n",
    "    exif_data = piexif.load(image.info['exif'])\n",
    "    orientation = exif_data['0th'].get(piexif.ImageIFD.Orientation, 1)\n",
    "    \n",
    "    # Correct orientation for image2 and image3\n",
    "    image2 = correct_orientation_opencv(image2, orientation)\n",
    "    image3 = correct_orientation_opencv(image3, orientation)\n",
    "    \n",
    "    # Normalize image3 (depth map)\n",
    "    image3_normalized = cv2.normalize(image3, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    blue_channel, green_channel, red_channel = extract_rgb_channels(image_cv)\n",
    "    \n",
    "    # Convert the original image to grayscale\n",
    "    grayscale_image = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Check the resolution of the depth map and upscale if necessary\n",
    "    if image3.shape != TARGET_SIZE:\n",
    "        print(f\"Depth map size {image3.shape} does not match target size {TARGET_SIZE}. Upscaling required.\")\n",
    "        upscaled_image3 = upscale_image(image3, TARGET_SIZE)\n",
    "    else:\n",
    "        upscaled_image3 = image3\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(f\"Original image shape: {image_cv.shape}\")\n",
    "    print(f\"Image 2 shape: {image2.shape}\")\n",
    "    print(f\"Image 3 shape: {image3.shape}\")\n",
    "    print(f\"Normalized Image 3 shape: {image3_normalized.shape}\")\n",
    "    print(f\"Blue channel shape: {blue_channel.shape}\")\n",
    "    print(f\"Green channel shape: {green_channel.shape}\")\n",
    "    print(f\"Red channel shape: {red_channel.shape}\")\n",
    "    print(f\"Grayscale image shape: {grayscale_image.shape}\")\n",
    "    print(f\"Upscaled Image 3 shape: {upscaled_image3.shape}\")\n",
    "    \n",
    "    # Report all image sizes without raising an error\n",
    "    report_image_sizes([image_cv, image2, image3, blue_channel, green_channel, red_channel, grayscale_image, upscaled_image3])\n",
    "    \n",
    "    save_images(image_cv, image2, image3_normalized, blue_channel, green_channel, red_channel, grayscale_image, upscaled_image3, os.path.basename(image_path), save_directory, timestamp)\n",
    "    display_images(image_cv, image2, image3_normalized, blue_channel, green_channel, red_channel, grayscale_image, upscaled_image3)\n",
    "    \n",
    "    # Create and save the 4-channel image\n",
    "    four_channel_image = merge_channels_to_4_channel(image_cv, upscaled_image3)\n",
    "    save_four_channel_image(four_channel_image, os.path.basename(image_path), save_directory, timestamp)\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    print(f\"Total processing time: {total_end_time - total_start_time:.2f} seconds\")\n",
    "\n",
    "def process_all_images_in_folder(folder_path, save_directory):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            main(image_path, save_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_images_in_folder(source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ea105",
   "metadata": {},
   "source": [
    "Processing image: C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\02.Labelled_jpg\\11530_IMG_0666_509_500USA_09CAR_SEA_CARRIERXXX_2RGT_E_ORIG.JPG\n",
    "Extracting MP images using exiftool...\n",
    "MPImage2 saved to C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\03.ExtractedLayers\\20240730000050_11530_IMG_0666_509_500USA_09CAR_SEA_CARRIERXXX_2RGT_E_ORIG_mp_image2.png\n",
    "MPImage3 saved to C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\03.ExtractedLayers\\20240730000050_11530_IMG_0666_509_500USA_09CAR_SEA_CARRIERXXX_2RGT_E_ORIG_mp_image3.png\n",
    "Loading image...\n",
    "Loading image took 0.00 seconds\n",
    "Checking and correcting image orientation...\n",
    "Orientation check took 0.20 seconds\n",
    "Converting image to OpenCV format...\n",
    "Conversion took 0.06 seconds\n",
    "Extracting RGB channels...\n",
    "RGB extraction took 0.03 seconds\n",
    "Depth map size (768, 576) does not match target size (4284, 5712). Upscaling required.\n",
    "Upscaling image...\n",
    "Upscaling took 0.01 seconds\n",
    "Original image shape: (5712, 4284, 3)\n",
    "Image 2 shape: (2856, 2142)\n",
    "Image 3 shape: (768, 576)\n",
    "Normalized Image 3 shape: (768, 576)\n",
    "Blue channel shape: (5712, 4284)\n",
    "Green channel shape: (5712, 4284)\n",
    "Red channel shape: (5712, 4284)\n",
    "Grayscale image shape: (5712, 4284)\n",
    "Upscaled Image 3 shape: (5712, 4284)\n",
    "Image 1 has shape (2856, 2142), expected (5712, 4284)\n",
    "Image 2 has shape (768, 576), expected (5712, 4284)\n",
    "Saving images...\n",
    "Saving images took 1.47 seconds\n",
    "Displaying images...\n",
    "\n",
    "Displaying images took 5.48 seconds\n",
    "Merging channels to create a 4-channel image...\n",
    "Merging channels took 0.14 seconds\n",
    "Saving the 4-channel image...\n",
    "4-channel image saved to C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\03.ExtractedLayers\\20240730000050_11530_IMG_0666_509_500USA_09CAR_SEA_CARRIERXXX_2RGT_E_ORIG_four_channel.png\n",
    "Total processing time: 8.75 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650be84",
   "metadata": {},
   "source": [
    "# Part 5 : Resize (Crop) files and Select by Channel\n",
    "\n",
    "This processes only greyscale, original and four_channel files.\n",
    "\n",
    "All the photos were taken in the inner 1/9th frame in the camera.  It is therefore possible to crop every file into the middle part.  (imagine 2 vertical and 2 horizontal lines at 1/3 and 2/3 along each side framing each piece in the middle)\n",
    "\n",
    "\n",
    "TIMING\n",
    "\n",
    "This can take 2hrs 20 + (not multi-thread)\n",
    "\n",
    "\n",
    "LOCALISATION\n",
    "\n",
    "source_dir     where the extracted layers are\n",
    "\n",
    "dest_dir       where the resized or cropped files will go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa604c7",
   "metadata": {},
   "source": [
    "# WARNING this can take 7806 seconds i.e.  2hr 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Define directories\n",
    "source_dir = Path(r\"C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\03.ExtractedLayers\")\n",
    "dest_dir = Path(r\"C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\04.ReSized\")\n",
    "\n",
    "# Create destination directory if it doesn't exist\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clear destination directory\n",
    "for file in dest_dir.glob('*'):\n",
    "    if file.is_file():\n",
    "        file.unlink()\n",
    "\n",
    "# File endings to process\n",
    "file_endings = {\n",
    "    \"grayscale.png\": \"gs_\",\n",
    "    \"original.png\": \"or_\",\n",
    "    \"four_channel.png\": \"fc_\"\n",
    "}\n",
    "\n",
    "# Function to process an image\n",
    "def process_image(file_path, save_path):\n",
    "    with Image.open(file_path) as img:\n",
    "        width, height = img.size\n",
    "        # Crop to the center third\n",
    "        left = width // 3\n",
    "        right = left * 2\n",
    "        top = height // 3\n",
    "        bottom = top * 2\n",
    "        cropped_img = img.crop((left, top, right, bottom))\n",
    "        # Save the processed image\n",
    "        cropped_img.save(save_path)\n",
    "\n",
    "# Function to get new filename\n",
    "def get_new_filename(file_path, prefix, suffix=\"9th\"):\n",
    "    return f\"{prefix}{file_path.stem}_{suffix}{file_path.suffix}\"\n",
    "\n",
    "# Start overall timing\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# Dictionary to store timing for each file type\n",
    "timings = {key: 0 for key in file_endings.keys()}\n",
    "\n",
    "# Count files before processing\n",
    "file_count_before = sum(1 for file in source_dir.rglob('*') if file.is_file() and any(file.name.endswith(ending) for ending in file_endings.keys()))\n",
    "\n",
    "# Process each file\n",
    "for file in source_dir.rglob('*'):\n",
    "    if file.is_file():\n",
    "        for ending, prefix in file_endings.items():\n",
    "            if file.name.endswith(ending):\n",
    "                new_filename = get_new_filename(file, prefix)\n",
    "                new_file_path = dest_dir / new_filename\n",
    "                \n",
    "                start_time = time.time()\n",
    "                process_image(file, new_file_path)\n",
    "                end_time = time.time()\n",
    "                timings[ending] += end_time - start_time\n",
    "\n",
    "# Count files after processing\n",
    "file_count_after = sum(1 for file in dest_dir.rglob('*') if file.is_file())\n",
    "\n",
    "# End overall timing\n",
    "overall_end_time = time.time()\n",
    "overall_elapsed_time = overall_end_time - overall_start_time\n",
    "\n",
    "# Output results\n",
    "print(f\"Overall time taken: {overall_elapsed_time:.2f} seconds\")\n",
    "print(f\"Files before processing: {file_count_before}\")\n",
    "print(f\"Files after processing: {file_count_after}\")\n",
    "\n",
    "# Output timing for each file type\n",
    "for ending, time_taken in timings.items():\n",
    "    print(f\"Time taken for files ending with '{ending}': {time_taken:.2f} seconds\")\n",
    "\n",
    "# Additional checks\n",
    "if file_count_before == file_count_after:\n",
    "    print(\"All files were successfully processed and transferred.\")\n",
    "else:\n",
    "    print(\"Some files may not have been processed correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2b88e",
   "metadata": {},
   "source": [
    "Overall time taken: 7806.32 seconds\n",
    "Files before processing: 4590\n",
    "Files after processing: 4590\n",
    "Time taken for files ending with 'grayscale.png': 4177.89 seconds\n",
    "Time taken for files ending with 'original.png': 1684.26 seconds\n",
    "Time taken for files ending with 'four_channel.png': 1899.27 seconds\n",
    "All files were successfully processed and transferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7a49b",
   "metadata": {},
   "source": [
    "# Part 6 : Generating classification Folders\n",
    "\n",
    "This moves the relevant files into folders related to their data channel and then their classification type, with the files being placed in subfolders of the classification\n",
    "e.g.\n",
    "      06.DataSets_gy         >   country            >  100RUS\n",
    "      image type=greyscale       classification        named class\n",
    "\n",
    "There are 3 image types:\n",
    "      greyscale\n",
    "      original\n",
    "      four-channel\n",
    "\n",
    "There are 4 classifications:\n",
    "      country            what 'country' is the piece?  This can be determined by the colour or sculpt.  (5 classes)\n",
    "      force              is the type of the piece land, air or sea?  (3 classes)\n",
    "      piece              which of the 9 piece types is it? (9 classes)\n",
    "      exact_piece        what is the country - piece combination (45 classes)\n",
    "\n",
    "\n",
    "LOCALISATION (in each of the subparts)\n",
    "\n",
    "src_folder = folder where the cropped data went\n",
    "dst_folders = {\n",
    "    'fc': folder for four-channel\n",
    "    'gs': folder for greyscale\n",
    "    'or': folder for original\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298e24a",
   "metadata": {},
   "source": [
    "# Part 6.1 : split country classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abfc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='file_copy.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the source and destination directories\n",
    "src_folder = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\04.ReSized'\n",
    "dst_folders = {\n",
    "    'fc': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\08.DataSets_fc',\n",
    "    'gs': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\06.DataSets_gy',\n",
    "    'or': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\07.DataSets_or'\n",
    "}\n",
    "\n",
    "# Country codes\n",
    "country_codes = ['100RUS', '200GER', '300UK', '400JAP', '500USA']\n",
    "\n",
    "# Create country subfolders in destination directories\n",
    "for key, dst_folder in dst_folders.items():\n",
    "    country_folder = os.path.join(dst_folder, 'country')\n",
    "    os.makedirs(country_folder, exist_ok=True)\n",
    "    for code in country_codes:\n",
    "        os.makedirs(os.path.join(country_folder, code), exist_ok=True)\n",
    "\n",
    "# Function to copy files to their respective directories\n",
    "def copy_files():\n",
    "    # Count the files to ensure correct distribution\n",
    "    count_files = defaultdict(int)\n",
    "    for file_name in os.listdir(src_folder):\n",
    "        if file_name.endswith('.png'):\n",
    "            prefix = file_name.split('_')[0]\n",
    "            if prefix in dst_folders:\n",
    "                parts = file_name.split('_')\n",
    "                if len(parts) > 6:\n",
    "                    country_code = parts[6]\n",
    "                    if country_code in country_codes:\n",
    "                        dst_folder = os.path.join(dst_folders[prefix], 'country', country_code)\n",
    "                        src_file = os.path.join(src_folder, file_name)\n",
    "                        dst_file = os.path.join(dst_folder, file_name)\n",
    "                        try:\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "                            logging.info(f\"Copied {src_file} to {dst_file}\")\n",
    "                            count_files[prefix] += 1\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error copying {src_file} to {dst_file}: {e}\")\n",
    "\n",
    "    # Verify the counts\n",
    "    for prefix, count in count_files.items():\n",
    "        expected_count = 1530\n",
    "        if count != expected_count:\n",
    "            logging.warning(f\"{prefix} files count mismatch. Expected {expected_count}, found {count}.\")\n",
    "        else:\n",
    "            logging.info(f\"All {prefix} files copied successfully: {count} files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        copy_files()\n",
    "        logging.info(\"File copying completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586fc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "718b0898",
   "metadata": {},
   "source": [
    "# Part 6.2 : Split into Force classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='file_copy.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the source and destination directories\n",
    "src_folder = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\04.ReSized'\n",
    "dst_folders = {\n",
    "    'fc': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\08.DataSets_fc',\n",
    "    'gs': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\06.DataSets_gy',\n",
    "    'or': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\07.DataSets_or'\n",
    "}\n",
    "\n",
    "# Force classifications\n",
    "force_classifications = ['LND', 'SEA', 'AIR']\n",
    "\n",
    "# Create force subfolders in destination directories\n",
    "for key, dst_folder in dst_folders.items():\n",
    "    force_folder = os.path.join(dst_folder, 'force')\n",
    "    os.makedirs(force_folder, exist_ok=True)\n",
    "    for classification in force_classifications:\n",
    "        os.makedirs(os.path.join(force_folder, classification), exist_ok=True)\n",
    "\n",
    "# Function to copy files to their respective directories\n",
    "def copy_files():\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Count the files to ensure correct distribution\n",
    "    count_files = defaultdict(int)\n",
    "    for file_name in os.listdir(src_folder):\n",
    "        if file_name.endswith('.png'):\n",
    "            prefix = file_name.split('_')[0]\n",
    "            if prefix in dst_folders:\n",
    "                parts = file_name.split('_')\n",
    "                if len(parts) > 8:\n",
    "                    force_class = parts[8]\n",
    "                    if force_class in force_classifications:\n",
    "                        dst_folder = os.path.join(dst_folders[prefix], 'force', force_class)\n",
    "                        src_file = os.path.join(src_folder, file_name)\n",
    "                        dst_file = os.path.join(dst_folder, file_name)\n",
    "                        try:\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "                            logging.info(f\"Copied {src_file} to {dst_file}\")\n",
    "                            count_files[prefix] += 1\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error copying {src_file} to {dst_file}: {e}\")\n",
    "\n",
    "    # Verify the counts\n",
    "    for prefix, count in count_files.items():\n",
    "        expected_count = 1530\n",
    "        if count != expected_count:\n",
    "            logging.warning(f\"{prefix} files count mismatch. Expected {expected_count}, found {count}.\")\n",
    "        else:\n",
    "            logging.info(f\"All {prefix} files copied successfully: {count} files.\")\n",
    "\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info(f\"File copying completed successfully in {elapsed_time:.2f} seconds.\")\n",
    "    print(f\"File copying completed successfully in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        copy_files()\n",
    "        logging.info(\"File copying completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb6346",
   "metadata": {},
   "source": [
    "File copying completed successfully in 12.31 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05ae60",
   "metadata": {},
   "source": [
    "# Part 6.3 : Split into piece classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4583fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='file_copy.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the source and destination directories\n",
    "src_folder = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\04.ReSized'\n",
    "dst_folders = {\n",
    "    'fc': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\08.DataSets_fc',\n",
    "    'gs': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\06.DataSets_gy',\n",
    "    'or': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\07.DataSets_or'\n",
    "}\n",
    "\n",
    "# Piece classifications\n",
    "piece_classifications = ['01INF', '02TNK', '03FGT', '04BMB', '05DES', '06TRS', '07SUB', '08BAT', '09CAR']\n",
    "\n",
    "# Create piece subfolders in destination directories\n",
    "for key, dst_folder in dst_folders.items():\n",
    "    piece_folder = os.path.join(dst_folder, 'piece')\n",
    "    os.makedirs(piece_folder, exist_ok=True)\n",
    "    for classification in piece_classifications:\n",
    "        os.makedirs(os.path.join(piece_folder, classification), exist_ok=True)\n",
    "\n",
    "# Function to copy files to their respective directories\n",
    "def copy_files():\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Count the files to ensure correct distribution\n",
    "    count_files = defaultdict(int)\n",
    "    for file_name in os.listdir(src_folder):\n",
    "        if file_name.endswith('.png'):\n",
    "            prefix = file_name.split('_')[0]\n",
    "            if prefix in dst_folders:\n",
    "                parts = file_name.split('_')\n",
    "                if len(parts) > 7:\n",
    "                    piece_class = parts[7]\n",
    "                    if piece_class in piece_classifications:\n",
    "                        dst_folder = os.path.join(dst_folders[prefix], 'piece', piece_class)\n",
    "                        src_file = os.path.join(src_folder, file_name)\n",
    "                        dst_file = os.path.join(dst_folder, file_name)\n",
    "                        try:\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "                            logging.info(f\"Copied {src_file} to {dst_file}\")\n",
    "                            count_files[prefix] += 1\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error copying {src_file} to {dst_file}: {e}\")\n",
    "\n",
    "    # Verify the counts\n",
    "    for prefix, count in count_files.items():\n",
    "        expected_count = 1530  # 1530 files per prefix, distributed among 9 classifications\n",
    "        if count != expected_count:\n",
    "            logging.warning(f\"{prefix} files count mismatch. Expected {expected_count}, found {count}.\")\n",
    "        else:\n",
    "            logging.info(f\"All {prefix} files copied successfully: {count} files.\")\n",
    "\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info(f\"File copying completed successfully in {elapsed_time:.2f} seconds.\")\n",
    "    print(f\"File copying completed successfully in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        copy_files()\n",
    "        logging.info(\"File copying completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4860143",
   "metadata": {},
   "source": [
    "\n",
    "File copying completed successfully in 12.48 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc455cfc",
   "metadata": {},
   "source": [
    "# Part 6.4 : split into exact piece classification (piece and country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1020223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='file_copy.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the source and destination directories\n",
    "src_folder = r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\04.ReSized'\n",
    "dst_folders = {\n",
    "    'fc': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\08.DataSets_fc',\n",
    "    'gs': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\06.DataSets_gy',\n",
    "    'or': r'C:\\Users\\ReCas\\OneDrive\\Documents\\2024_AIMachineLearning\\99_Projects\\07.DataSets_or'\n",
    "}\n",
    "\n",
    "# Exact piece classifications\n",
    "exact_piece_classifications = [\n",
    "    '101_100RUS_01INF', '102_100RUS_02TNK', '103_100RUS_03FGT', '104_100RUS_04BMB', '105_100RUS_05DES', \n",
    "    '106_100RUS_06TRS', '107_100RUS_07SUB', '108_100RUS_08BAT', '109_100RUS_09CAR', '201_200GER_01INF', \n",
    "    '202_200GER_02TNK', '203_200GER_03FGT', '204_200GER_04BMB', '205_200GER_05DES', '206_200GER_06TRS', \n",
    "    '207_200GER_07SUB', '208_200GER_08BAT', '209_200GER_09CAR', '301_300UK_01INF', '302_300UK_02TNK', \n",
    "    '303_300UK_03FGT', '304_300UK_04BMB', '305_300UK_05DES', '306_300UK_06TRS', '307_300UK_07SUB', \n",
    "    '308_300UK_08BAT', '309_300UK_09CAR', '401_400JAP_01INF', '402_400JAP_02TNK', '403_400JAP_03FGT', \n",
    "    '404_400JAP_04BMB', '405_400JAP_05DES', '406_400JAP_06TRS', '407_400JAP_07SUB', '408_400JAP_08BAT', \n",
    "    '409_400JAP_09CAR', '501_500USA_01INF', '502_500USA_02TNK', '503_500USA_03FGT', '504_500USA_04BMB', \n",
    "    '505_500USA_05DES', '506_500USA_06TRS', '507_500USA_07SUB', '508_500USA_08BAT', '509_500USA_09CAR'\n",
    "]\n",
    "\n",
    "# Create exact_piece subfolders in destination directories\n",
    "for key, dst_folder in dst_folders.items():\n",
    "    exact_piece_folder = os.path.join(dst_folder, 'exact_piece')\n",
    "    os.makedirs(exact_piece_folder, exist_ok=True)\n",
    "    for classification in exact_piece_classifications:\n",
    "        os.makedirs(os.path.join(exact_piece_folder, classification), exist_ok=True)\n",
    "\n",
    "# Function to copy files to their respective directories\n",
    "def copy_files():\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Count the files to ensure correct distribution\n",
    "    count_files = defaultdict(int)\n",
    "    for file_name in os.listdir(src_folder):\n",
    "        if file_name.endswith('.png'):\n",
    "            prefix = file_name.split('_')[0]\n",
    "            if prefix in dst_folders:\n",
    "                parts = file_name.split('_')\n",
    "                if len(parts) > 5:\n",
    "                    exact_piece_class = '_'.join(parts[5:8])  # Combine the 6th, 7th, and 8th parts\n",
    "                    if exact_piece_class in exact_piece_classifications:\n",
    "                        dst_folder = os.path.join(dst_folders[prefix], 'exact_piece', exact_piece_class)\n",
    "                        src_file = os.path.join(src_folder, file_name)\n",
    "                        dst_file = os.path.join(dst_folder, file_name)\n",
    "                        try:\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "                            logging.info(f\"Copied {src_file} to {dst_file}\")\n",
    "                            count_files[prefix] += 1\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error copying {src_file} to {dst_file}: {e}\")\n",
    "\n",
    "    # Verify the counts\n",
    "    expected_count = 34\n",
    "    for prefix in dst_folders.keys():\n",
    "        for classification in exact_piece_classifications:\n",
    "            classification_folder = os.path.join(dst_folders[prefix], 'exact_piece', classification)\n",
    "            actual_count = len(os.listdir(classification_folder))\n",
    "            if actual_count != expected_count:\n",
    "                logging.warning(f\"{prefix} {classification} files count mismatch. Expected {expected_count}, found {actual_count}.\")\n",
    "            else:\n",
    "                logging.info(f\"All {prefix} {classification} files copied successfully: {actual_count} files.\")\n",
    "\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info(f\"File copying completed successfully in {elapsed_time:.2f} seconds.\")\n",
    "    print(f\"File copying completed successfully in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        copy_files()\n",
    "        logging.info(\"File copying completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94dd8d",
   "metadata": {},
   "source": [
    "File copying completed successfully in 14.04 seconds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
